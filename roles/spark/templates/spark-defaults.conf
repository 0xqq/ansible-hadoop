# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
#主节点设置,多个节点高可用
spark.master                     spark://{% for host in groups['masters'] %}{% if not loop.last %}{{host}}:7077,{%else%}{{host}}:7077{% endif %}{% endfor %}
##Whether to log Spark events, useful for reconstructing the Web UI after the application has finished.
spark.eventLog.enabled             true
spark.eventLog.dir                 {{spark.spark_defaults.spark_eventlog_dir}}
spark.yarn.historyServer.address   {{spark.spark_defaults.spark_yarn_historyserver_address}}
spark.history.ui.port              {{spark.spark_defaults.spark_history_ui_port}}
spark.eventLog.compress            false
spark.serializer                    org.apache.spark.serializer.KryoSerializer
##Spark应用程序Application所占的内存大小，这里的Driver对应Yarn中的ApplicationMaster；
spark.driver.memory                {{spark.spark_defaults.spark_driver_memory}} 
## Amount of memory to use per executor process, in the same format as JVM memory strings (e.g. 512m, 2g).对应的container
spark.executor.memory              {{spark.spark_defaults.spark_executor_memory}}
spark.storage.memoryFraction       0.66
spark.shuffle.spill                true
spark.shuffle.spill.compress       true


spark.executor.extraJavaOptions  -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark
##Directory to use for "scratch" space in Spark, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks。NOTE: In Spark 1.0 and later this will be overriden by SPARK_LOCAL_DIRS (Standalone, Mesos) or LOCAL_DIRS (YARN) environment variables set by the cluster manager.设置多个分散io
spark.local.dir                    {{spark.spark_defaults.spark_local_dir}}
##Port for your application's dashboard, which shows memory and workload data
spark.ui.port                      4040
##job运行期间的压缩的相关设置
spark.broadcast.compress           true
spark.rdd.compress                 false
spark.io.compression.codec         snappy
spark.io.compression.snappy.blockSize    32768
##Default number of tasks to use across the cluster for distributed shuffle operations (groupByKey, reduceByKey, etc) when not set by user.
spark.default.parallelism            {{spark.spark_defaults.spark_default_parallelism}}
spark.dynamicAllocation.minExecutors {{spark.spark_defaults.spark_dynamicallocation_minexecutors}}
spark.dynamicAllocation.maxExecutors {{spark.spark_defaults.spark_dynamicallocation_maxexecutors}}
spark.akka.frameSize                1000
spark.shuffle.service.enabled       true
spark.dynamicAllocation.enabled     true
##shuffle阶段合并小文件
#spark.shuffle.consolidateFiles     true
